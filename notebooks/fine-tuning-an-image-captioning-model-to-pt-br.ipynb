{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289},{"sourceId":12268821,"sourceType":"datasetVersion","datasetId":7731226}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The goal of this project is to fine-tune an existing image captioning model to generate captions in Brazilian Portuguese. In order to do that, the Flickr8k database is used and the following steps are performed:\n\n1. Translate captions using a translation model\n2. Fine-tune an existing image captioning model\n3. Evaluate model performance","metadata":{}},{"cell_type":"markdown","source":"# 1. Importing Libraries","metadata":{}},{"cell_type":"markdown","source":"Installing required libraries which are not available by default on Kaggle","metadata":{}},{"cell_type":"code","source":"!pip install evaluate\n!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T23:12:25.776607Z","iopub.execute_input":"2025-06-30T23:12:25.776889Z","iopub.status.idle":"2025-06-30T23:12:36.021508Z","shell.execute_reply.started":"2025-06-30T23:12:25.776869Z","shell.execute_reply":"2025-06-30T23:12:36.020551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import pipeline\nfrom transformers import (\n    BlipProcessor,\n    BlipForConditionalGeneration,\n    TrainingArguments,\n    Trainer,\n)\nimport evaluate  # biblioteca Hugging Face Evaluate\nimport nltk\nfrom torch.utils.data import RandomSampler\nimport warnings\n\nnltk.download('punkt')\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T22:47:56.706556Z","iopub.execute_input":"2025-06-30T22:47:56.706865Z","iopub.status.idle":"2025-06-30T22:47:58.724410Z","shell.execute_reply.started":"2025-06-30T22:47:56.706836Z","shell.execute_reply":"2025-06-30T22:47:58.723740Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Step A - Translating captions to Brazilian Portuguese","metadata":{}},{"cell_type":"code","source":"# loading captions file\ncaptions_file = \"/kaggle/input/flickr8k/captions.txt\"\n#captions_file = \"/kaggle/input/flickr8k-portuguese-captions/captions_pt.csv\" # uncomment if captions were already translated\ncaptions_df = pd.read_csv(captions_file)\n\ncaptions_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T16:24:07.035520Z","iopub.execute_input":"2025-06-30T16:24:07.036072Z","iopub.status.idle":"2025-06-30T16:24:07.256244Z","shell.execute_reply.started":"2025-06-30T16:24:07.036051Z","shell.execute_reply":"2025-06-30T16:24:07.255605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# setting up the translation pipeline\ntranslator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-big-en-pt\")\n\n# translating\ncaptions_df[\"caption_pt\"] = captions_df[\"caption\"].apply(\n    lambda x: translator(x)[0][\"translation_text\"]\n)\n\n# saving file\ncaptions_df.to_csv(\"/kaggle/working/captions_pt.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check translated content\ncaptions_pt_df = pd.read_csv(\"/kaggle/working/captions_pt.csv\")\n#captions_pt_df = captions_df # uncomment if captions already translated\ncaptions_pt_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T16:24:07.256923Z","iopub.execute_input":"2025-06-30T16:24:07.257212Z","iopub.status.idle":"2025-06-30T16:24:07.264686Z","shell.execute_reply.started":"2025-06-30T16:24:07.257183Z","shell.execute_reply":"2025-06-30T16:24:07.263882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Step B - Fine-Tuning Image Captioning Model","metadata":{}},{"cell_type":"markdown","source":"First, generate a `Dataset`","metadata":{}},{"cell_type":"code","source":"# loading translated captions\ncaptions = pd.read_csv(\"/kaggle/working/captions_pt.csv\")  # Must have columns: image, caption_pt\n#captions = captions_df # uncomment if captions already translated\n\n# get image file path for each image\ncaptions[\"image\"] = captions[\"image\"].apply(lambda x: f\"/kaggle/input/flickr8k/Images/{x}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T16:24:07.266522Z","iopub.execute_input":"2025-06-30T16:24:07.266727Z","iopub.status.idle":"2025-06-30T16:24:07.296757Z","shell.execute_reply.started":"2025-06-30T16:24:07.266711Z","shell.execute_reply":"2025-06-30T16:24:07.296042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate dataset\n\n# helper function\ndef preprocess(example):\n    example[\"pil_image\"] = Image.open(example[\"image\"]).convert(\"RGB\")\n    return example\n\ndataset = Dataset.from_pandas(captions)\ndataset = dataset.map(preprocess, remove_columns = [\"image\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T16:24:07.297395Z","iopub.execute_input":"2025-06-30T16:24:07.297587Z","iopub.status.idle":"2025-06-30T17:38:29.525034Z","shell.execute_reply.started":"2025-06-30T16:24:07.297569Z","shell.execute_reply":"2025-06-30T17:38:29.524433Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Second, loading model to be fine-tuned","metadata":{}},{"cell_type":"code","source":"# loading model and processor\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T17:38:29.525812Z","iopub.execute_input":"2025-06-30T17:38:29.526314Z","iopub.status.idle":"2025-06-30T17:38:36.562735Z","shell.execute_reply.started":"2025-06-30T17:38:29.526295Z","shell.execute_reply":"2025-06-30T17:38:36.562215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Third, preparing and performing the training","metadata":{}},{"cell_type":"code","source":"# collation function\ndef collate_fn(batch):\n    images = [item[\"pil_image\"] for item in batch]\n    texts = [item[\"caption_pt\"] for item in batch]\n    \n    # process image and captions\n    inputs = processor(images, texts, return_tensors=\"pt\", padding=\"longest\", truncation=True)\n\n    return {\n        \"pixel_values\": inputs[\"pixel_values\"],\n        \"input_ids\": inputs[\"input_ids\"], \n        \"labels\": inputs[\"input_ids\"],     # labels = input_ids to allow autoregressive training\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T17:38:36.563422Z","iopub.execute_input":"2025-06-30T17:38:36.563710Z","iopub.status.idle":"2025-06-30T17:38:36.568303Z","shell.execute_reply.started":"2025-06-30T17:38:36.563684Z","shell.execute_reply":"2025-06-30T17:38:36.567733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# defining training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=4,\n    num_train_epochs=3,\n    save_strategy=\"steps\",\n    save_steps=100,\n    save_total_limit=2,\n    learning_rate=5e-5,\n    remove_unused_columns=False,\n    eval_strategy=\"no\",\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    report_to=\"none\"\n)\n\n# train the model\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n    data_collator=collate_fn,\n)\n\nprint(\"Start training...\")\ntrainer.train()\nprint(\"Training completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T17:38:36.568979Z","iopub.execute_input":"2025-06-30T17:38:36.569166Z","iopub.status.idle":"2025-06-30T22:14:04.925502Z","shell.execute_reply.started":"2025-06-30T17:38:36.569151Z","shell.execute_reply":"2025-06-30T22:14:04.923917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# saving model\nmodel.save_pretrained('./results/fine_tuned_blip_pt')\nprocessor.save_pretrained('./results/fine_tuned_blip_pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T22:14:10.397123Z","iopub.execute_input":"2025-06-30T22:14:10.397428Z","iopub.status.idle":"2025-06-30T22:14:13.256793Z","shell.execute_reply.started":"2025-06-30T22:14:10.397410Z","shell.execute_reply":"2025-06-30T22:14:13.256213Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Testing Fine-Tuned Model","metadata":{}},{"cell_type":"code","source":"# loading fine-tuned model\nprocessor = BlipProcessor.from_pretrained('./results/fine_tuned_blip_pt')\nmodel = BlipForConditionalGeneration.from_pretrained('./results/fine_tuned_blip_pt')\n\n# generating a caption in pt-BR for a sample image\nimg = Image.open(\"/kaggle/input/flickr8k/Images/1020651753_06077ec457.jpg\").convert(\"RGB\")\ninputs = processor(img, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    ids = model.generate(**inputs, max_new_tokens=30)\n\ncaption = processor.decode(ids[0], skip_special_tokens=True)\nprint(caption)  # prints generated caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T22:39:44.560920Z","iopub.execute_input":"2025-06-30T22:39:44.561231Z","iopub.status.idle":"2025-06-30T22:39:48.624392Z","shell.execute_reply.started":"2025-06-30T22:39:44.561205Z","shell.execute_reply":"2025-06-30T22:39:48.623696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# showing sample image to verify\nimg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T22:41:37.773749Z","iopub.execute_input":"2025-06-30T22:41:37.773999Z","iopub.status.idle":"2025-06-30T22:41:37.821186Z","shell.execute_reply.started":"2025-06-30T22:41:37.773982Z","shell.execute_reply":"2025-06-30T22:41:37.820636Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Evaluate using BLEU score","metadata":{}},{"cell_type":"code","source":"# getting fine-tuned model and captions path\nmodel_path = \"./results/fine_tuned_blip_pt\"\ncaptions_path = \"/kaggle/input/flickr8k-portuguese-captions/captions_pt.csv\"\n\n# loading model and processor\nprocessor = BlipProcessor.from_pretrained(model_path)\nmodel = BlipForConditionalGeneration.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# loading dataset\ndf = pd.read_csv(captions_path)\ndf[\"image\"] = df[\"image\"].apply(lambda x: f\"/kaggle/input/flickr8k/Images/{x}\")  # ajustar se necessário\ndataset = Dataset.from_pandas(df)\n\n# function to generate caption\ndef generate_caption(image_path):\n    image = Image.open(image_path).convert(\"RGB\")\n    inputs = processor(image, return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        output = model.generate(**inputs, max_new_tokens=30)\n    return processor.decode(output[0], skip_special_tokens=True)\n\n# preparing data to evaluation\nreferences = []\npredictions = []\n\nfor example in dataset.select(range(100)):  # using a few samples to perform comparisons\n    gold_caption = example[\"caption_pt\"]\n    image_path = example[\"image\"]\n\n    pred_caption = generate_caption(image_path)\n\n    references.append([gold_caption.lower()])\n    predictions.append(pred_caption.lower())\n\n# computing BLEU score\nbleu = evaluate.load(\"bleu\")\nresults = bleu.compute(predictions=predictions, references=references)\n\nprint(f\"BLEU score: {results['bleu']:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T23:07:17.046952Z","iopub.execute_input":"2025-06-30T23:07:17.047284Z","iopub.status.idle":"2025-06-30T23:08:00.239622Z","shell.execute_reply.started":"2025-06-30T23:07:17.047261Z","shell.execute_reply":"2025-06-30T23:08:00.238995Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Evaluate with other metrics","metadata":{}},{"cell_type":"code","source":"meteor = evaluate.load(\"meteor\")\nrouge = evaluate.load(\"rouge\")\n\nmeteor_score = meteor.compute(predictions=predictions, references=references)\nrouge_score = rouge.compute(predictions=predictions, references=[r[0] for r in references])  # ROUGE espera string\n\nprint(f\"METEOR score: {meteor_score['meteor']:.4f}\")\nprint(f\"ROUGE-L score: {rouge_score['rougeL']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T23:12:47.053386Z","iopub.execute_input":"2025-06-30T23:12:47.053923Z","iopub.status.idle":"2025-06-30T23:12:51.274166Z","shell.execute_reply.started":"2025-06-30T23:12:47.053892Z","shell.execute_reply":"2025-06-30T23:12:51.273582Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Exporting fine tuned model to be used in app","metadata":{}},{"cell_type":"code","source":"!zip -r mymodel.zip ./results/fine_tuned_blip_pt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T22:20:34.366779Z","iopub.execute_input":"2025-06-30T22:20:34.367641Z","iopub.status.idle":"2025-06-30T22:21:25.779026Z","shell.execute_reply.started":"2025-06-30T22:20:34.367607Z","shell.execute_reply":"2025-06-30T22:21:25.778349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}